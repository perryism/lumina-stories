# Quick Setup Examples - Copy one of these configurations to .env

# ============================================
# OPTION 1: LM Studio (Local Server)
# ============================================
# 1. Download LM Studio from https://lmstudio.ai/
# 2. Download a model (e.g., mistral-7b-instruct)
# 3. Start the local server in LM Studio
# 4. Use this configuration:

AI_PROVIDER=local
LOCAL_API_URL=http://localhost:1234/v1
LOCAL_MODEL=mistral-7b-instruct-v0.2

# ============================================
# OPTION 2: Ollama (Local Server)
# ============================================
# 1. Install Ollama from https://ollama.ai/
# 2. Run: ollama pull mistral
# 3. Use this configuration:

# AI_PROVIDER=local
# LOCAL_API_URL=http://localhost:11434/v1
# LOCAL_MODEL=mistral

# ============================================
# OPTION 3: OpenAI (Cloud API)
# ============================================
# 1. Get API key from https://platform.openai.com/api-keys
# 2. Use this configuration:

# AI_PROVIDER=openai
# OPENAI_API_KEY=sk-your-key-here

# ============================================
# OPTION 4: Google Gemini (Cloud API)
# ============================================
# 1. Get API key from https://aistudio.google.com/app/apikey
# 2. Use this configuration:

# AI_PROVIDER=gemini
# GEMINI_API_KEY=your-key-here

# ============================================
# ADVANCED: Multiple Models for Different Tasks
# ============================================
# Use a fast model for outlines/summaries and a better model for chapters:

# AI_PROVIDER=local
# LOCAL_API_URL=http://localhost:1234/v1
# LOCAL_MODEL_OUTLINE=mistral-7b-instruct
# LOCAL_MODEL_SUMMARY=mistral-7b-instruct
# LOCAL_MODEL_CHAPTER=llama-3-70b-instruct

