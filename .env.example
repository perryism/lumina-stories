# AI Provider Configuration
# Choose between "gemini", "openai", or "local"
AI_PROVIDER=gemini

# Gemini API Configuration
# Get your API key from: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here
# Legacy support (will use GEMINI_API_KEY if not set)
API_KEY=your_gemini_api_key_here

# OpenAI API Configuration
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Local Chat Server Configuration
# For use with LM Studio, Ollama, LocalAI, or any OpenAI-compatible local server
LOCAL_API_URL=http://localhost:1234/v1
LOCAL_API_KEY=not-needed
# Model name(s) - comma-delimited list of available models
# The first model in the list will be used by default
# You can switch between models using the dropdown in the UI
LOCAL_MODEL=mistral-7b-instruct,llama-3-8b-instruct,local-model
# Optional: Specify different models for different tasks
# LOCAL_MODEL_OUTLINE=fast-model
# LOCAL_MODEL_CHAPTER=creative-model
# LOCAL_MODEL_SUMMARY=fast-model

# Backend API Server Configuration
# URL of the Express.js backend server for template/story management
# Default: http://localhost:3001
# For Docker: http://api:3001
API_SERVER_URL=http://localhost:3001

